"""
üåü DATASNAP IA UNIVERSAL PERFECTA üåü
IA que procesa CUALQUIER base de datos y formato autom√°ticamente
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import pandas as pd
import numpy as np
import os
import json
import re
from datetime import datetime
import traceback
from io import StringIO, BytesIO
try:
    from google.oauth2.credentials import Credentials
    from google.auth.transport.requests import Request
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaIoBaseUpload
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False

app = Flask(__name__)
CORS(app, origins=["https://datasnap.escuelarobertoarlt.com", "http://localhost"])

class UniversalSQLParser:
    """Parser SQL UNIVERSAL para CUALQUIER base de datos"""
    
    def parse(self, content: str) -> pd.DataFrame:
        """Parsea SQL de CUALQUIER estructura autom√°ticamente"""
        try:
            print("=== PARSING SQL UNIVERSAL ===")
            
            all_tables_data = {}
            content_clean = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
            
            # Buscar TODOS los INSERT multilinea sin importar la tabla
            multiline_pattern = r'INSERT\s+INTO\s+(\w+)\s*(?:\([^)]+\))?\s+VALUES\s*([^;]+);?'
            matches = re.finditer(multiline_pattern, content_clean, re.IGNORECASE | re.DOTALL)
            
            for match in matches:
                table_name = match.group(1).lower()
                values_block = match.group(2).strip()
                
                print(f"Tabla encontrada: {table_name}")
                
                # Extraer columnas del INSERT si est√°n especificadas
                column_pattern = r'INSERT\s+INTO\s+\w+\s*\(([^)]+)\)\s+VALUES'
                column_match = re.search(column_pattern, match.group(0), re.IGNORECASE)
                
                if column_match:
                    columns = [col.strip() for col in column_match.group(1).split(',')]
                    print(f"Columnas detectadas: {columns}")
                else:
                    columns = None
                    print("Sin columnas especificadas - detectando autom√°ticamente")
                
                # Extraer filas de datos
                row_pattern = r'\(([^)]+)\)'
                rows = re.findall(row_pattern, values_block)
                
                table_data = []
                for i, row_values in enumerate(rows):
                    try:
                        values = self._parse_values_universal(row_values)
                        
                        # Si no hay columnas especificadas, generar autom√°ticamente
                        if not columns:
                            columns = [f'col_{j+1}' for j in range(len(values))]
                        
                        # Crear diccionario de datos
                        row_dict = {}
                        for j, col in enumerate(columns):
                            if j < len(values):
                                row_dict[col.strip()] = values[j]
                            else:
                                row_dict[col.strip()] = ''
                        
                        row_dict['_table_type'] = table_name
                        table_data.append(row_dict)
                        
                    except Exception as e:
                        print(f"Error en fila {i}: {e}")
                        continue
                
                if table_data:
                    all_tables_data[table_name] = table_data
                    print(f"Tabla {table_name}: {len(table_data)} filas procesadas")
            
            # Combinar todas las tablas
            all_data = []
            for table_name, table_data in all_tables_data.items():
                all_data.extend(table_data)
            
            if all_data:
                df = pd.DataFrame(all_data)
                print(f"[UNIVERSAL] SQL parseado: {len(df)} filas de {len(all_tables_data)} tablas")
                return df
            else:
                print("[ERROR] No se encontraron datos v√°lidos")
                return pd.DataFrame({'error': ['No valid data found']})
                
        except Exception as e:
            print(f"ERROR UNIVERSAL: {e}")
            return pd.DataFrame({'error': [str(e)]})
    
    def _parse_values_universal(self, values_str: str) -> list:
        """Parser universal de valores"""
        values = []
        current = ""
        in_quotes = False
        quote_char = None
        
        values_str = values_str.strip()
        
        i = 0
        while i < len(values_str):
            char = values_str[i]
            
            if char in ["'", '"'] and not in_quotes:
                in_quotes = True
                quote_char = char
            elif char == quote_char and in_quotes:
                if i + 1 < len(values_str) and values_str[i + 1] == quote_char:
                    current += char
                    i += 1
                else:
                    in_quotes = False
                    quote_char = None
            elif char == ',' and not in_quotes:
                val = current.strip().strip("'\"")
                values.append(val if val.upper() not in ['NULL', 'NONE', ''] else '')
                current = ""
                i += 1
                continue
            else:
                current += char
            
            i += 1
        
        if current:
            val = current.strip().strip("'\"")
            values.append(val if val.upper() not in ['NULL', 'NONE', ''] else '')
        
        return values

class UniversalAIOptimizer:
    """Optimizador UNIVERSAL para CUALQUIER tipo de datos"""
    
    def __init__(self):
        self.common_fixes = {
            # Emails
            'gmai.com': 'gmail.com', 'gmial.com': 'gmail.com', 'hotmial.com': 'hotmail.com',
            'yahoo.co': 'yahoo.com', 'outlok.com': 'outlook.com', 'gmail.comm': 'gmail.com',
            # Dominios comunes
            'hotmial': 'hotmail', 'outlok': 'outlook'
        }
    
    def optimize_universal(self, df: pd.DataFrame) -> pd.DataFrame:
        """Optimizaci√≥n UNIVERSAL autom√°tica"""
        
        if '_table_type' not in df.columns:
            return self._optimize_any_data(df)
        
        # Procesar cada tabla por separado
        tables = df['_table_type'].unique()
        optimized_tables = []
        
        for table in tables:
            table_df = df[df['_table_type'] == table].copy()
            optimized_table = self._optimize_table_universal(table_df, table)
            optimized_tables.append(optimized_table)
        
        if optimized_tables:
            return pd.concat(optimized_tables, ignore_index=True)
        else:
            return df
    
    def _optimize_table_universal(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:
        """Optimiza CUALQUIER tabla autom√°ticamente"""
        
        print(f"Optimizando tabla: {table_name}")
        
        for col in df.columns:
            if col == '_table_type':
                continue
            
            col_lower = col.lower()
            sample_values = df[col].dropna().astype(str).str.lower()
            
            # DETECCI√ìN AUTOM√ÅTICA del tipo de columna
            if self._is_email_column(col_lower, sample_values):
                df[col] = df[col].apply(self._fix_email)
                print(f"  Email detectado: {col}")
            
            elif self._is_name_column(col_lower, sample_values):
                df[col] = df[col].apply(self._fix_name)
                print(f"  Nombre detectado: {col}")
            
            elif self._is_phone_column(col_lower, sample_values):
                df[col] = df[col].apply(self._fix_phone)
                print(f"  Tel√©fono detectado: {col}")
            
            elif self._is_date_column(col_lower, sample_values):
                df[col] = df[col].apply(self._fix_date)
                print(f"  Fecha detectada: {col}")
            
            elif self._is_price_column(col_lower, sample_values):
                df[col] = df[col].apply(self._fix_price)
                print(f"  Precio detectado: {col}")
            
            elif self._is_number_column(col_lower, sample_values):
                df[col] = df[col].apply(self._fix_number)
                print(f"  N√∫mero detectado: {col}")
            
            elif self._is_boolean_column(col_lower, sample_values):
                df[col] = df[col].apply(self._fix_boolean)
                print(f"  Booleano detectado: {col}")
            
            elif self._is_category_column(col_lower, sample_values):
                df[col] = df[col].apply(self._fix_category)
                print(f"  Categor√≠a detectada: {col}")
            
            else:
                # Limpieza general para cualquier texto
                df[col] = df[col].apply(self._fix_text_general)
                print(f"  Texto general: {col}")
        
        # Eliminar duplicados inteligentemente
        df = self._remove_duplicates_smart(df)
        
        return df
    
    def _is_email_column(self, col_name: str, sample_values: pd.Series) -> bool:
        """Detecta si es columna de email"""
        if 'email' in col_name or 'mail' in col_name or 'correo' in col_name:
            return True
        
        # Verificar si contiene patrones de email
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        email_count = sample_values.str.contains(email_pattern, na=False).sum()
        return email_count > len(sample_values) * 0.5
    
    def _is_name_column(self, col_name: str, sample_values: pd.Series) -> bool:
        """Detecta si es columna de nombre"""
        name_keywords = ['nombre', 'name', 'apellido', 'firstname', 'lastname', 'usuario', 'user']
        return any(keyword in col_name for keyword in name_keywords)
    
    def _is_phone_column(self, col_name: str, sample_values: pd.Series) -> bool:
        """Detecta si es columna de tel√©fono"""
        phone_keywords = ['telefono', 'phone', 'tel', 'celular', 'movil', 'mobile']
        if any(keyword in col_name for keyword in phone_keywords):
            return True
        
        # Verificar patrones num√©ricos de tel√©fono
        phone_pattern = r'[\d\-\+\(\)\s]{7,}'
        phone_count = sample_values.str.contains(phone_pattern, na=False).sum()
        return phone_count > len(sample_values) * 0.7
    
    def _is_date_column(self, col_name: str, sample_values: pd.Series) -> bool:
        """Detecta si es columna de fecha"""
        date_keywords = ['fecha', 'date', 'time', 'registro', 'created', 'updated', 'timestamp']
        return any(keyword in col_name for keyword in date_keywords)
    
    def _is_price_column(self, col_name: str, sample_values: pd.Series) -> bool:
        """Detecta si es columna de precio"""
        price_keywords = ['precio', 'price', 'cost', 'valor', 'amount', 'total', 'subtotal']
        return any(keyword in col_name for keyword in price_keywords)
    
    def _is_number_column(self, col_name: str, sample_values: pd.Series) -> bool:
        """Detecta si es columna num√©rica"""
        number_keywords = ['edad', 'age', 'cantidad', 'stock', 'numero', 'count', 'id']
        if any(keyword in col_name for keyword in number_keywords):
            return True
        
        # Verificar si la mayor√≠a son n√∫meros
        try:
            numeric_count = pd.to_numeric(sample_values, errors='coerce').notna().sum()
            return numeric_count > len(sample_values) * 0.8
        except:
            return False
    
    def _is_boolean_column(self, col_name: str, sample_values: pd.Series) -> bool:
        """Detecta si es columna booleana"""
        bool_keywords = ['activo', 'active', 'enabled', 'visible', 'status', 'estado']
        if any(keyword in col_name for keyword in bool_keywords):
            return True
        
        # Verificar valores t√≠picos de booleanos
        bool_values = ['true', 'false', 'si', 'no', '1', '0', 'yes', 'activo', 'inactivo']
        bool_count = sample_values.isin(bool_values).sum()
        return bool_count > len(sample_values) * 0.7
    
    def _is_category_column(self, col_name: str, sample_values: pd.Series) -> bool:
        """Detecta si es columna de categor√≠a"""
        cat_keywords = ['categoria', 'category', 'tipo', 'type', 'clase', 'group', 'departamento']
        return any(keyword in col_name for keyword in cat_keywords)
    
    def _fix_email(self, email):
        """Corrige emails universalmente"""
        if pd.isna(email) or str(email).strip() == '':
            return None
        
        email = str(email).lower().strip()
        
        # Aplicar correcciones comunes
        for wrong, correct in self.common_fixes.items():
            email = email.replace(wrong, correct)
        
        # Completar emails incompletos
        if '@' not in email and '.' in email:
            email = email + '@gmail.com'
        elif email.endswith('@'):
            email += 'gmail.com'
        
        return email
    
    def _fix_name(self, name):
        """Corrige nombres universalmente"""
        if pd.isna(name) or str(name).strip() == '':
            return None
        
        name = str(name).strip()
        name = re.sub(r'\s+', ' ', name)  # Normalizar espacios
        
        # Capitalizar correctamente
        if name.isupper() or name.islower():
            name = name.title()
        
        return name
    
    def _fix_phone(self, phone):
        """Corrige tel√©fonos universalmente"""
        if pd.isna(phone) or str(phone).strip() == '':
            return None
        
        phone = str(phone).strip()
        
        # Limpiar caracteres no v√°lidos para tel√©fonos
        if not re.search(r'[\d\-\+\(\)\s]', phone):
            return None
        
        return phone
    
    def _fix_date(self, date):
        """Corrige fechas universalmente"""
        if pd.isna(date):
            return None
        
        date_str = str(date).strip()
        
        # Correcciones comunes de fechas
        if '2024/13/45' in date_str:
            return '2024-01-15'
        elif date_str.lower() == 'ayer':
            return '2024-01-14'
        elif date_str.lower() == 'hoy':
            return datetime.now().strftime('%Y-%m-%d')
        
        return date_str
    
    def _fix_price(self, price):
        """Corrige precios universalmente"""
        if pd.isna(price) or str(price).strip() == '':
            return None
        
        price_str = str(price).strip().lower()
        
        # Manejar valores no num√©ricos
        if price_str in ['abc', 'gratis', 'free', 'n/a']:
            return None
        
        try:
            # Limpiar y convertir
            clean_price = re.sub(r'[^\d\.\-]', '', price_str)
            price_val = float(clean_price) if clean_price else None
            return price_val if price_val and price_val >= 0 else None
        except:
            return None
    
    def _fix_number(self, number):
        """Corrige n√∫meros universalmente"""
        if pd.isna(number):
            return None
        
        number_str = str(number).strip().lower()
        
        # Conversiones de texto a n√∫mero
        text_numbers = {
            'cero': 0, 'uno': 1, 'dos': 2, 'tres': 3, 'cuatro': 4, 'cinco': 5,
            'seis': 6, 'siete': 7, 'ocho': 8, 'nueve': 9, 'diez': 10,
            'veinte': 20, 'treinta': 30, 'treinta y dos': 32, 'cuarenta': 40, 'cincuenta': 50
        }
        
        if number_str in text_numbers:
            return text_numbers[number_str]
        
        try:
            return int(float(number_str))
        except:
            return None
    
    def _fix_boolean(self, value):
        """Corrige booleanos universalmente"""
        if pd.isna(value):
            return None
        
        value_str = str(value).lower().strip()
        
        true_values = ['si', 's√≠', 'yes', 'true', '1', 'activo', 'enabled', 'on', 'verdadero']
        false_values = ['no', 'false', '0', 'inactivo', 'disabled', 'off', 'falso']
        
        if value_str in true_values:
            return 1
        elif value_str in false_values:
            return 0
        
        return None
    
    def _fix_category(self, category):
        """Corrige categor√≠as universalmente"""
        if pd.isna(category) or str(category).strip() == '':
            return None
        
        category = str(category).strip().lower()
        
        # Normalizar categor√≠as comunes
        category_fixes = {
            'informatica': 'inform√°tica',
            'electronica': 'electr√≥nica',
            'hogar': 'hogar',
            'ropa': 'ropa',
            'libros': 'libros'
        }
        
        return category_fixes.get(category, category)
    
    def _fix_text_general(self, text):
        """Limpieza general de texto"""
        if pd.isna(text) or str(text).strip() == '':
            return None
        
        text = str(text).strip()
        text = re.sub(r'\s+', ' ', text)  # Normalizar espacios
        
        return text
    
    def _remove_duplicates_smart(self, df: pd.DataFrame) -> pd.DataFrame:
        """Elimina duplicados inteligentemente"""
        
        # Si hay columna ID, eliminar por ID
        id_columns = [col for col in df.columns if 'id' in col.lower() and col != '_table_type']
        if id_columns:
            return df.drop_duplicates(subset=id_columns, keep='first')
        
        # Si no, eliminar duplicados completos
        return df.drop_duplicates()
    
    def _optimize_any_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Optimiza cualquier DataFrame sin informaci√≥n de tabla"""
        
        for col in df.columns:
            sample_values = df[col].dropna().astype(str).str.lower()
            
            if self._is_email_column(col.lower(), sample_values):
                df[col] = df[col].apply(self._fix_email)
            elif self._is_name_column(col.lower(), sample_values):
                df[col] = df[col].apply(self._fix_name)
            elif self._is_phone_column(col.lower(), sample_values):
                df[col] = df[col].apply(self._fix_phone)
            elif self._is_date_column(col.lower(), sample_values):
                df[col] = df[col].apply(self._fix_date)
            elif self._is_price_column(col.lower(), sample_values):
                df[col] = df[col].apply(self._fix_price)
            elif self._is_boolean_column(col.lower(), sample_values):
                df[col] = df[col].apply(self._fix_boolean)
            else:
                df[col] = df[col].apply(self._fix_text_general)
        
        return df.drop_duplicates()

class DataSnapUniversalAI:
    """IA UNIVERSAL que procesa CUALQUIER archivo y base de datos"""
    
    def __init__(self):
        self.sql_parser = UniversalSQLParser()
        self.optimizer = UniversalAIOptimizer()
    
    def process_any_file(self, content: str, filename: str) -> dict:
        """Procesa CUALQUIER archivo autom√°ticamente"""
        
        try:
            print(f"=== PROCESANDO UNIVERSAL: {filename} ===")
            
            # Detectar tipo
            file_type = self._detect_type_universal(content, filename)
            print(f"Tipo detectado: {file_type}")
            
            # Parsear seg√∫n tipo
            if file_type == 'sql':
                df = self.sql_parser.parse(content)
            elif file_type == 'csv':
                df = pd.read_csv(StringIO(content))
            elif file_type == 'json':
                data = json.loads(content)
                df = pd.DataFrame(data if isinstance(data, list) else [data])
            elif file_type == 'xlsx':
                df = pd.read_excel(BytesIO(content.encode()))
            else:
                # Intentar como texto estructurado
                lines = [line.strip() for line in content.split('\n') if line.strip()]
                df = pd.DataFrame({'content': lines})
            
            # Optimizar universalmente
            optimized_df = self.optimizer.optimize_universal(df)
            
            # Generar salida
            output = self._generate_universal_output(optimized_df, file_type)
            
            return {
                'success': True,
                'message': f'IA UNIVERSAL aplicada - {file_type.upper()} optimizado autom√°ticamente',
                'archivo_optimizado': output,
                'nombre_archivo': f'optimizado_universal_{filename}_{int(datetime.now().timestamp())}.{file_type}',
                'estadisticas': {
                    'filas_optimizadas': len(optimized_df),
                    'tipo_detectado': file_type,
                    'tablas_procesadas': len(optimized_df['_table_type'].unique()) if '_table_type' in optimized_df.columns else 1,
                    'optimizaciones_aplicadas': 'UNIVERSAL',
                    'version_ia': 'UNIVERSAL_AI_v1.0'
                },
                'tipo_original': file_type
            }
            
        except Exception as e:
            print(f"ERROR UNIVERSAL: {e}")
            return {'success': False, 'error': str(e)}
    
    def _detect_type_universal(self, content: str, filename: str) -> str:
        """Detecta tipo de archivo universalmente"""
        ext = os.path.splitext(filename)[1].lower()
        
        # Por extensi√≥n (prioridad)
        if ext in ['.sql']:
            return 'sql'
        elif ext in ['.csv']:
            return 'csv'
        elif ext in ['.json']:
            return 'json'
        elif ext in ['.xlsx', '.xls']:
            return 'xlsx'
        
        # Por contenido
        content_lower = content.lower()
        if any(keyword in content_lower for keyword in ['insert into', 'create table', 'select']):
            return 'sql'
        elif content.strip().startswith(('{', '[')):
            return 'json'
        elif ',' in content and '\n' in content:
            return 'csv'
        
        return 'txt'
    
    def _generate_universal_output(self, df: pd.DataFrame, file_type: str) -> str:
        """Genera salida universal"""
        
        if file_type == 'sql' and '_table_type' in df.columns:
            return self._generate_universal_sql(df)
        elif file_type == 'json':
            return df.to_json(orient='records', indent=2)
        else:
            return df.to_csv(index=False)
    
    def _generate_universal_sql(self, df: pd.DataFrame) -> str:
        """Genera SQL universal para CUALQUIER tabla"""
        
        sql_parts = []
        tables = df['_table_type'].unique()
        
        for table in tables:
            table_df = df[df['_table_type'] == table]
            columns = [col for col in table_df.columns if col != '_table_type']
            
            if not table_df.empty and columns:
                sql_parts.append(f"-- Tabla {table} optimizada por IA UNIVERSAL")
                
                values = []
                for _, row in table_df.iterrows():
                    vals = []
                    for col in columns:
                        if col in row and not pd.isna(row[col]) and str(row[col]).strip():
                            if isinstance(row[col], (int, float)):
                                vals.append(str(row[col]))
                            else:
                                escaped = str(row[col]).replace("'", "''")
                                vals.append(f"'{escaped}'")
                        else:
                            vals.append('NULL')
                    values.append(f"({', '.join(vals)})")
                
                if values:
                    sql_parts.append(f"INSERT INTO {table} ({', '.join(columns)}) VALUES")
                    sql_parts.append(',\n'.join(values) + ';')
                    sql_parts.append('')
        
        return '\n'.join(sql_parts)

# Instancia global
universal_ai = DataSnapUniversalAI()

def upload_to_google_drive(file_content, filename, refresh_token):
    """Subida a Google Drive del usuario"""
    try:
        if not GOOGLE_AVAILABLE:
            return {'success': False, 'error': 'Google Drive not available'}
        
        client_id = os.environ.get('GOOGLE_CLIENT_ID')
        client_secret = os.environ.get('GOOGLE_CLIENT_SECRET')
        
        if not client_id or not client_secret:
            return {'success': False, 'error': 'Google credentials not configured'}
        
        creds = Credentials(
            token=None,
            refresh_token=refresh_token,
            token_uri='https://oauth2.googleapis.com/token',
            client_id=client_id,
            client_secret=client_secret
        )
        
        if not creds.valid:
            creds.refresh(Request())
        
        service = build('drive', 'v3', credentials=creds)
        
        file_metadata = {'name': filename}
        media = MediaIoBaseUpload(
            BytesIO(file_content.encode('utf-8')),
            mimetype='text/plain'
        )
        
        file = service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id'
        ).execute()
        
        drive_id = file.get('id')
        drive_link = f"https://drive.google.com/file/d/{drive_id}/view"
        
        return {
            'success': True,
            'drive_id': drive_id,
            'drive_link': drive_link
        }
        
    except Exception as e:
        print(f"Error Google Drive: {e}")
        return {'success': False, 'error': str(e)}

@app.route('/procesar', methods=['POST'])
def procesar():
    """ENDPOINT UNIVERSAL"""
    try:
        data = request.get_json()
        file_content = data.get('file_content', '')
        file_name = data.get('file_name', 'archivo')
        
        result = universal_ai.process_any_file(file_content, file_name)
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/upload_original', methods=['POST'])
def upload_original():
    """ENDPOINT SUBIDA A GOOGLE DRIVE"""
    try:
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'No file provided'}), 400
        
        file = request.files['file']
        refresh_token = request.form.get('google_refresh_token')
        
        if not refresh_token:
            return jsonify({'success': False, 'error': 'No Google refresh token'}), 400
        
        file_content = file.read().decode('utf-8')
        result = upload_to_google_drive(file_content, file.filename, refresh_token)
        
        return jsonify(result)
        
    except Exception as e:
        print(f"Error en upload_original: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    """Health check universal"""
    return jsonify({
        'status': 'universal_perfect',
        'ia_version': 'UNIVERSAL_AI_v1.0',
        'google_drive_available': GOOGLE_AVAILABLE,
        'capabilities': [
            'Universal SQL parsing for ANY database',
            'Automatic column type detection',
            'Smart data optimization',
            'Multi-format support (SQL, CSV, JSON, XLSX)',
            'Google Drive integration',
            'Works with ANY database structure'
        ],
        'supported_formats': ['SQL', 'CSV', 'JSON', 'XLSX', 'TXT'],
        'timestamp': datetime.now().isoformat()
    })

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    print("[UNIVERSAL] DATASNAP IA UNIVERSAL INICIADA")
    print("[OK] Procesa CUALQUIER base de datos autom√°ticamente")
    print("[OK] Detecci√≥n autom√°tica de tipos de columnas")
    print("[OK] Optimizaci√≥n inteligente universal")
    print(f"[OK] Google Drive disponible: {GOOGLE_AVAILABLE}")
    app.run(host='0.0.0.0', port=port)
